{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn, warnings, contextlib, IPython, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is built on the notebook submitted for assignment one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a small utility for printing progress - there are some long operations later one and it is handy to have an indicator.\n",
    "<br>(It does slow down the loop though, which is annoying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, update_display, Markdown\n",
    "\n",
    "class Progress:\n",
    "    def __init__(self, name, count):\n",
    "        self.name = name\n",
    "        self.count = count\n",
    "        self._display(\"Starting '\" + name + \"'\")\n",
    "        self.last_update = 0\n",
    "        self.step = math.ceil(self.count/100) # Update when about 1% has elapsed.\n",
    "    \n",
    "    def update(self, value):\n",
    "        if value != self.count and int(value - self.last_update) % self.step > 0:\n",
    "            return # Don't update too often since that will slow down loops a lot!\n",
    "        self.last_update = value\n",
    "        p = (value+1)/self.count\n",
    "        self._refresh(f\"{self.name}: {int(100*p)}% complete.\")\n",
    "        \n",
    "    def complete(self):\n",
    "        self._refresh(f\"{self.name}: 100% complete.\")\n",
    "        \n",
    "    def _display(self, message):\n",
    "        display(Markdown(message), display_id=str(id(self)))\n",
    "        \n",
    "    def _refresh(self, message):\n",
    "        update_display(Markdown(message), display_id=str(id(self)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a small utility for summarizing warnings, since those tend to take up a lot of space and aren't always something one wants to avoid.\n",
    "<br>The idea is that one would disable it when looking to debug them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def tally_warnings():\n",
    "    with warnings.catch_warnings(record=True) as caught:\n",
    "        \n",
    "        yield\n",
    "        \n",
    "        tally = defaultdict(int)\n",
    "        for warning in caught:\n",
    "            tally[warning.category.__name__] += 1\n",
    "        \n",
    "        for category, amount in tally.items():\n",
    "            print(\"  \", category + \"s:\", amount, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting MyGaussianNB\n",
    "\n",
    "The code below is similar to what it was in assignment 1. \n",
    "\n",
    "I changed bits of it after reading the solution, I hadn't seen some tricks like the Counter class or using dict.get as a sorting key.\n",
    "\n",
    "In terms of adapting to missing values, I decided to let it fail gracefully without raising errors as far as possible. This is partially because failing gracefully is a trait of some Python libraries I've grown to like, but mainly because it makes it easier to experiment with later. I can draw graphs with 0 to 100% missing values without worrying that my estimator is going to throw a fit at some arbitrary point I define here.\n",
    "\n",
    "The fallbacks are as follows:\n",
    "  - In the training data, each feature ignores nan values.\n",
    "  - A feature with no valid training values will have invalid parameters.\n",
    "  - When predicting, features with invalid parameters are ignored. (the likelihood is unaffected by the feature)\n",
    "  - When predicting, invalid sample feature values are ignored. (the likelihood is unaffected by the feature)\n",
    "  - If no features affect the likelyhood, the likelihood is one, and only the prior is considered.\n",
    "    (In other words, the model eventually falls back to predicting the majority class)\n",
    "  - A nan in the target feature causes the entire record it is in to be ignored.\n",
    " \n",
    "In short, the model uses what features it can, and when there are no features it predicts the majority class.\n",
    "<br>The one thing it is not design to handle is the case where *none* of the training labels are valid. In that case, it has no class to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from collections import Counter\n",
    "\n",
    "class MyGaussianNB(BaseEstimator, ClassifierMixin):          \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        if np.isnan(y).all():\n",
    "            raise ValueError(\"No valid training labels given\")\n",
    "        \n",
    "        self.priors = dict()\n",
    "        self.means = dict()\n",
    "        self.vars = dict()\n",
    "        \n",
    "        n_records = len(y)\n",
    "        n_features = X.shape[1]\n",
    "        classes = Counter(y).items()\n",
    "        \n",
    "        for cls, count in classes:\n",
    "            \n",
    "            self.priors[cls] = count / n_records\n",
    "            \n",
    "            cls_data = X[y == cls]\n",
    "            self.means[cls] = np.zeros(n_features)\n",
    "            self.vars[cls] = np.zeros(n_features)\n",
    "            \n",
    "            for feature_index in range(n_features):\n",
    "                \n",
    "                feature_data = cls_data[:, feature_index]\n",
    "                \n",
    "                if np.isnan(feature_data).all():\n",
    "                    mean = np.nan\n",
    "                    var = np.nan\n",
    "                else:\n",
    "                    mean = np.nanmean(feature_data)\n",
    "                    var = np.nanvar(feature_data)\n",
    "                \n",
    "                self.means[cls][feature_index] = mean\n",
    "                self.vars[cls][feature_index] = var\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        predictions = np.zeros(len(X))\n",
    "        \n",
    "        for i, sample in enumerate(X):\n",
    "            \n",
    "            posteriors = dict()\n",
    "            \n",
    "            first = next(iter(self.priors.keys()))\n",
    "            for cls, prior in self.priors.items():\n",
    "\n",
    "                likelihood = 1\n",
    "                \n",
    "                for feature_index, feature_value in enumerate(sample):\n",
    "                    \n",
    "                    if np.isnan(feature_value):\n",
    "                        continue\n",
    "                    \n",
    "                    mean = self.means[cls][feature_index]\n",
    "                    var = self.vars[cls][feature_index]\n",
    "                    \n",
    "                    if np.isnan(mean) or np.isnan(var):\n",
    "                        continue\n",
    "                    \n",
    "                    x = feature_value\n",
    "                    \n",
    "                    if var == 0:\n",
    "                        p = 1 if x == mean else 0\n",
    "\n",
    "                    else:\n",
    "                        a = (var * 2 * np.pi)**-0.5\n",
    "                        b = -(x - mean)**2 / (2 * var)\n",
    "                        \n",
    "                        p = a*np.exp(b)\n",
    "\n",
    "                    likelihood *= p\n",
    "\n",
    "                posterior = prior * likelihood\n",
    "                posteriors[cls] = posterior\n",
    "                \n",
    "            predictions[i] = max(posteriors, key=posteriors.get)\n",
    "            \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data prep. Some of this is still manual, but I have discovered the joys of pipelines too.\n",
    "\n",
    "The steps are:\n",
    "\n",
    "  - read data\n",
    "  - filter classes and features for testing\n",
    "  - encoding\n",
    "  - make some of the data missing (\"nanification\")\n",
    "  - imputation (for the sklearn case only)\n",
    "  - normalization\n",
    " \n",
    "Reading, filtering, and encoding are done below. \n",
    "\n",
    "One Hot Encoding is used for the categorical descriptive features.  The target feature is given an ordinal encoding, which isn't needed - plain labels would have been fine. However, it works neatly with certain graphs and also prevents pd.get_dummies from affecting it, so I'm leaving it be.\n",
    "\n",
    "The ability to exclude features is there for exploration/experimentation - the penguins dataset in particular behaves very differently when the categorical features are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "def encode(data, target_feature):\n",
    "    \n",
    "    encoder = OrdinalEncoder()\n",
    "    data[[target_feature]] = encoder\\\n",
    "        .fit_transform(data[[target_feature]])\n",
    "    \n",
    "    data = pd.get_dummies(data)\n",
    "    \n",
    "    return data\n",
    "  \n",
    "def read_data(path, target_feature, \n",
    "              allowed_classes=None, allowed_features=None,\n",
    "             verbose=False):\n",
    "    \n",
    "    data = pd.read_csv(path, index_col = 0)  \n",
    "    \n",
    "    if verbose:\n",
    "        display(data.head(2))\n",
    "    \n",
    "    if allowed_classes is not None:\n",
    "        allowed_rows  = data[target_feature].isin(allowed_classes)\n",
    "        data = data[allowed_rows]\n",
    "        data = data.reset_index(drop=True)\n",
    "        \n",
    "    if allowed_features is not None:\n",
    "        allowed_features = set(allowed_features)\n",
    "        allowed_features.add(target_feature)\n",
    "        data = data[allowed_features]\n",
    "    \n",
    "    if verbose:\n",
    "        display(data.head(2))\n",
    "    \n",
    "    data = encode(data, target_feature)\n",
    "    \n",
    "    # Move the target feature to the end of the dataset.\n",
    "    # The exact position is unimportant, but having it in a fixed position\n",
    "    # means that it can be easily identified after the column names are lost.\n",
    "    \n",
    "    features = list(data.columns)\n",
    "    features.remove(target_feature)\n",
    "    features.insert(0, target_feature)\n",
    "    data = data[features]\n",
    "    \n",
    "    if verbose:\n",
    "        display(data.head(2))\n",
    "\n",
    "    raw_data = data.to_numpy()    \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test/example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = read_data(\n",
    "    path=\"penguins_af.csv\", \n",
    "    target_feature=\"species\", \n",
    "    allowed_classes=[\"Adelie\",\"Chinstrap\"],\n",
    "    allowed_features=[\"island\", \"bill_length_mm\", \"body_mass_g\", \"year\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned data, displayed below, has not been split into X and y components yet. This is so that an imputation can be applied which acts across the whole dataset at once, such as KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indexing when splitting it is a little awkward and depends on how the read_data function works, so I've defined  a function with it here.\n",
    "\n",
    "\"Boxing\" below refers to having y as a 2D array as opposed to a 1D array. It can be useful to have in 2D for pipelines and transforms, but it needs to be in 1D when actually used as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data, box_y=False):\n",
    "    \n",
    "    X = data[:,1:]\n",
    "    \n",
    "    if box_y:\n",
    "        y = data[:,0:1]\n",
    "    else:\n",
    "        y = data[:,0]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def unbox(y):\n",
    "    \n",
    "    return y[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a transform for making random subsets of the data NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from numpy.random import choice\n",
    "\n",
    "def nanify(X, p):\n",
    "    \n",
    "    N = len(X)\n",
    "    M = int(N*p)\n",
    "    \n",
    "    X = X.copy()\n",
    "    X[choice(N, M, replace=False)] = np.nan\n",
    "    \n",
    "    return X\n",
    "\n",
    "class Nanifier(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, p=0.5, p0=None):\n",
    "        \n",
    "        if p0 is None:\n",
    "            p0 = p\n",
    "        \n",
    "        self.p = p\n",
    "        self.p0 = p0\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        X = X.copy()\n",
    "        \n",
    "        for i in range(X.shape[1]):\n",
    "            p = self.p0 if i == 0 else self.p\n",
    "            X[:,i] = nanify(X[:,i], p)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of the above, displayed with spy plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spy(X):\n",
    "    \n",
    "    X = X.transpose()\n",
    "    \n",
    "    plt.figure(figsize=(10,2))\n",
    "    plt.axis(\"off\")\n",
    "    plt.spy((~np.isnan(X)).astype(int));\n",
    "    \n",
    "data = read_data(\"penguins_af.csv\", \"species\")\n",
    "X, y = split(data, box_y=True)\n",
    "\n",
    "for p in [0.1, 0.5, 0.9]:\n",
    "    \n",
    "    nf = Nanifier(p)\n",
    "    spy(nf.fit_transform(X))\n",
    "    spy(nf.fit_transform(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it's handy to have the penguins loaded in and named."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = read_data(\"penguins_af.csv\", \"species\")\n",
    "\n",
    "penguins_restricted = read_data(\"penguins_af.csv\", \"species\",\n",
    "    allowed_classes = [\"Adelie\",\"Chinstrap\"],\n",
    "    allowed_features = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all the data prep utility that is defined in advance - normalization and imputation are done using pipelines.\n",
    "\n",
    "A simple GNB setup using the above utilities would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "data = penguins\n",
    "\n",
    "combined_pl = make_pipeline(Nanifier(p=0.1), SimpleImputer(strategy=\"most_frequent\"))\n",
    "data = combined_pl.fit_transform(data, data)\n",
    "\n",
    "X, y = split(data)\n",
    "\n",
    "X_pl = make_pipeline(MinMaxScaler(), GaussianNB())\n",
    "pd.Series(cross_val_score(X_pl, X, y)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is relatively low, even with just 10% of values missing. \n",
    "\n",
    "That being said, it only scored ~70% in assignment 1 when all features and classes present. \n",
    "<br>If the features and classes are restricted, the model does better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "data = penguins_restricted\n",
    "\n",
    "combined_pl = make_pipeline(Nanifier(p=0.1), SimpleImputer(strategy=\"most_frequent\"))\n",
    "data = combined_pl.fit_transform(data, data)\n",
    "\n",
    "X, y = split(data)\n",
    "\n",
    "X_pl = make_pipeline(MinMaxScaler(), GaussianNB())\n",
    "pd.Series(cross_val_score(X_pl, X, y)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing new there, just sanity checking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the two strategies of dealing with missing values are compared. Imputation is used with SKLearn. Ignoring missing values can't be done through SKLearn, but the custom MyGNB has been adjusted to allow for it.\n",
    "\n",
    "These functions represent the experiments to be considered:\n",
    "  - Ingoring missing values using MyGNB.\n",
    "  - Univariate imputation, using the mean.\n",
    "  - Multivariate imputation, using KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "def ignore_missing(p):\n",
    "    \n",
    "    X_pl = make_pipeline(\n",
    "        Nanifier(p=p), \n",
    "        MinMaxScaler(),\n",
    "        MyGaussianNB()\n",
    "    )\n",
    "    \n",
    "    return X_pl\n",
    "\n",
    "def simple_imputation(p):\n",
    "    \n",
    "    X_pl = make_pipeline(\n",
    "        Nanifier(p=p), \n",
    "        SimpleImputer(strategy=\"mean\"),\n",
    "        MinMaxScaler(),\n",
    "        GaussianNB()\n",
    "    )\n",
    "    \n",
    "    return X_pl\n",
    "    \n",
    "def knn_imputation(p):\n",
    "    \n",
    "    X_pl = make_pipeline(\n",
    "        Nanifier(p=p), \n",
    "        KNNImputer(),\n",
    "        MinMaxScaler(),\n",
    "        GaussianNB()\n",
    "    )\n",
    "    \n",
    "    return X_pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function scores one or more of the above pipelines for different proportions of missing data. The default (5-fold as of writing this) cross validation is used.\n",
    "\n",
    "Note that the missing values transform is applied to the data as part of the preprocessing pipeline, and so is applied to both training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "def test_with(data, pipelines, samples=5):\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    \n",
    "    results = defaultdict(dict)\n",
    "    \n",
    "    X, y = split(data)\n",
    "    \n",
    "    progress = Progress(\"Scoring\", samples*len(pipelines))\n",
    "    \n",
    "    for i, pipeline in enumerate(pipelines):\n",
    "        for j, p in enumerate(np.linspace(0, 1, samples, endpoint=False)):\n",
    "\n",
    "            score = cross_val_score(pipeline(p), X, y)\n",
    "            score = pd.Series(score)\n",
    "            \n",
    "            results[pipeline.__name__][p] = score\n",
    "\n",
    "            progress.update(i*samples + j)\n",
    "            \n",
    "    progress.complete()\n",
    "    \n",
    "    for key, value in results.items():\n",
    "        results[key] = pd.DataFrame.from_dict(value)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a function to graph the output of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(results, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Proportion Missing\")\n",
    "    plt.ylabel(\"Cross-Val Score\")\n",
    "\n",
    "    colors = \"rbkm\"\n",
    "    for i, (name, result) in enumerate(results.items()):\n",
    "\n",
    "        line = result.mean()\n",
    "        style = \".\" + colors[i]\n",
    "\n",
    "        line.plot(kind=\"line\", style=style, label=name)\n",
    "\n",
    "    if len(results) > 1:\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a graph of how the sklearn model works with missing values.\n",
    "\n",
    "This is partially a sanity check, and partially to see how quickly the accuracy fades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_with(penguins, [simple_imputation], samples=200)\n",
    "graph(results, \"SImple Imputation, Penguins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a lovely trend, a little poetic - it implies that a bit of missing data strengthens the model.\n",
    "\n",
    "This trend is not replicated at all with the restricted penguins data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tally_warnings():\n",
    "    results = test_with(penguins_restricted, [simple_imputation], samples=200)\n",
    "\n",
    "graph(results, \"SImple Imputation, Penguins (Restricted)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The runtime warnings above occur very near the end, as there starts to be very little data left. It's not surprising that the numerical calculations break down at that point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the three on the restricted penguins dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    ignore_missing,\n",
    "    simple_imputation,\n",
    "    knn_imputation\n",
    "]\n",
    "\n",
    "with tally_warnings():\n",
    "    results = test_with(penguins_restricted, experiments, samples=50)\n",
    "\n",
    "graph(results, \"Penguins (Restricted)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They all do pretty similar. It seems like KNN has a slight edge over Simple Imputation, and Ignoring does slightl better than KNN. \n",
    "\n",
    "I'm not surprised KNN does better than the simple one, since it takes more data into account when imputing. I'm a little surprised at how well ignoring the data is doing, though one explanation there might be that relying and less, but more reliable, data is proving more effective in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, considering other datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_df(path, target_feature, samples=50):\n",
    "    \n",
    "    data = read_data(path, target_feature)\n",
    "    \n",
    "    with tally_warnings():\n",
    "        results = test_with(data, experiments, samples)\n",
    "\n",
    "    graph(results, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diabetes dataset shows a similar trend to the restricted penguins one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_df(\"diabetes.csv\", \"neg_pos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I kept the forecast one only as an example of how chaotic the results from the smaller datasets are - Atheletes and ApplesPears are similar.\n",
    "\n",
    "All three methods seem to do about the same here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_df(\"Forecast.csv\", \"Go-Out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glass V2 shows an increase in accuracy up to 50% missing data, before falling back. The accuracy is almost never above 50%. Near the end, ignoring does better than KNN which does better than simple imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_df(\"glassV2.csv\", \"Type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For MamMass, despite there being gaps between the models at any given point, I'd argue that the rate of loss in accuracy is more relevant here, and the three seem fairly even in that regard.\n",
    "\n",
    "Also, the change is quite small, the y-axis goes from 0.7 down to 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_df(\"MamMass.csv\", \"Severity\", samples=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MyGNB does better than the SKLearn one with no missing data, which makes comparing the methods of dealing with missing data unreliable here. It's a good reminder that there are other factors at play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_df(\"penguins_af.csv\", \"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know what to make of the survival one, the models do not seem to be drawing much information from the data at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_df(\"survival.csv\", \"Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vehicles and wine datasets also show the Ignoring > KNN > Simple trend near for missing proportions > ~60%. Simple imputation starts out stronger in the vehicles set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_df(\"vehicle.csv\", \"TYPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_df(\"wine.csv\", \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two methods perform very similarly. Which one wins out seems to depend on the dataset. With the ones above, when there is a lot of missing data (~70%), it seems ignoring the data works more often than not better than the others, with KNN second. Simple imputation works well for smaller amounts of missing data, up to around 40%.\n",
    "\n",
    "Imputing means some of the data will be wrong, but more of the correct data will be used. Ignoring means less of the data will be used, but what will be used is perhaps more reliable. (\"wrong\" and \"correct\" and \"reliable\" really does depend on the dataset) This really seems like one of those many, many, little controls that can be useful to tweak in specific situations, for fine tuning.\n",
    "\n",
    "It makes sense that SKLearn chose to support imputation and not the tailored fallbacks, given the mostly minor difference in performance. The fallbacks need to be carefully designed and implemented for every model anew, where as imputation is a separate preprocessing concern that can be engineered separately. This is also an interesting reminder that while SKLearn offers a lot in a very easy to access way, it is far from exhaustive and custom implementation and extension has the potential to further improve models.\n",
    "\n",
    "Finally, I'm still fascinated by the fact that some of the features in the penguins dataset diminish the SKLearn's NB model's ability to learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
